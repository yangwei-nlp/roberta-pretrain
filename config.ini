[Parameters]
epoch = 1
# 预训练遍历数据集次数

vocab_size = 21128
# 词典大小

train_files_path = /home/ai/yangwei/myResources/THUCNews
# 训练语料的目录

save_path = data/
# 模型权重保存目录

learning_rate = 1e-4
# 学习率

save_steps = 10000
# 训练每隔多少step保存模型参数

per_gpu_train_batch_size = 8
# 单个GPU的batch size

gradient_accumulation_steps = 32
# 梯度累积的步数(每隔多少次更新参数)

